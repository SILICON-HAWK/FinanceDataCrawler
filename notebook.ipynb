{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# URL of the webpage\n",
    "url = 'https://www.screener.in/company/SUNPHARMA/consolidated/'\n",
    "\n",
    "# Send HTTP request and get the HTML response\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Save raw HTML in a variable\n",
    "    html = response.text\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Rest of your code...\n",
    "else:\n",
    "    print(f\"Failed to retrieve webpage. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company_name: Sun Pharmaceuticals Industries Ltd\n",
      "Stock_price: 1,809 INR\n",
      "Percentage_change: -2.66%\n",
      "Ratios: {'Market Cap': '4,33,946 Cr. INR', 'Current Price': '1,809', 'High / Low': '1,960 / 1,140', 'Stock P/E': '38.7 INR', 'Book Value': '288 INR', 'Dividend Yield': '0.75 %', 'ROCE': '17.3 %', 'ROE': '16.7 %', 'Face Value': '1.00 INR'}\n",
      "About_and_key_points: About: Sun Pharmaceutical Industries Ltd is engaged in the business of manufacturing, developing and marketing a wide range of branded and generic formulations and Active Pharma Ingredients (APIs). The company and its subsidiaries has various manufacturing facilities spread across the world with trading and other incidental and related activities extending to global market.[1] It is the largest pharmaceutical company in India.[2] Key_points: Key points not found.\n",
      "Company_links: ['https://www.bseindia.com/bseplus/AnnualReport/524715/69342524715.pdf#page=193', 'https://sunpharma.com/wp-content/uploads/2021/07/SPIL-IR-Presentation-June-2021.pdf#page=4', 'https://sunpharma.com/products/', 'https://sunpharma.com/wp-content/uploads/2021/07/SPIL-IR-Presentation-June-2021.pdf#page=4', 'http://www.sunpharma.com', 'https://www.bseindia.com/stock-share-price/sun-pharmaceuticals-industries-ltd/SUNPHARMA/524715/', 'https://www.nseindia.com/get-quotes/equity?symbol=SUNPHARMA']\n",
      "Data stored in company_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_company_data(html):\n",
    "    \"\"\"\n",
    "    Extract company data from the given HTML.\n",
    "\n",
    "    Args:\n",
    "        html (str): The HTML content to parse.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted company data.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Parse HTML content\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Find the company-info div\n",
    "        company_info_div = soup.find('div', id='top')\n",
    "        if company_info_div is None:\n",
    "            print(\"Company info div not found.\")\n",
    "            return None\n",
    "\n",
    "        # Extract company information\n",
    "        company_name = company_info_div.find('h1', class_='margin-0')\n",
    "        company_name = company_name.text.strip() if company_name else \"Company name not found.\"\n",
    "\n",
    "        # Extract stock price and percentage change\n",
    "        stock_price_div = company_info_div.find('div', class_='font-size-18')\n",
    "        if stock_price_div:\n",
    "            stock_price = stock_price_div.find('span').text.strip().replace('\\n', '').replace('₹', '').replace('¹', '').replace('�','').replace('â‚', '').strip() + ' INR'\n",
    "            percentage_change = stock_price_div.find('span', class_='font-size-12').text.strip()\n",
    "        else:\n",
    "            stock_price = \"Stock price not found.\"\n",
    "            percentage_change = \"Percentage change not found.\"\n",
    "\n",
    "        # Extract market cap, current price, high/low, stock P/E, book value, dividend yield, ROCE, ROE, and face value\n",
    "        ratios_div = company_info_div.find('ul', id='top-ratios')\n",
    "        if ratios_div:\n",
    "            ratios = {}\n",
    "            for ratio in ratios_div.find_all('li'):\n",
    "                name = ratio.find('span', class_='name').text.strip()\n",
    "                value = ratio.find('span', class_='value').text.strip().replace('\\n', '').replace('₹', '').replace('¹', '').replace('�','').replace('â‚', '').strip()\n",
    "                # Remove extra whitespaces\n",
    "                value = ' '.join(value.split())\n",
    "                if 'Cr' in value:\n",
    "                    value = value + ' INR'\n",
    "                elif value.replace('.','',1).isdigit():\n",
    "                    value = value + ' INR'\n",
    "                ratios[name] = value\n",
    "        else:\n",
    "            ratios = {\"Ratios not found.\" : \"No data available.\"}\n",
    "\n",
    "        # Extract about, key points, and company links\n",
    "        company_data = {}\n",
    "        company_profile_div = company_info_div.find('div', class_='company-profile')\n",
    "        if company_profile_div:\n",
    "            about = company_profile_div.find('div', class_='sub').text.strip().replace('\\n', ' ') if company_profile_div.find('div', class_='sub') else \"About not found.\"\n",
    "            key_points = company_profile_div.find('div', class_='sub commentary').text.strip() if company_profile_div.find('div', class_='sub commentary') else \"Key points not found.\"\n",
    "            company_links = [link.get('href') for link in company_profile_div.find_all('a')]\n",
    "            company_data[\"about_and_key_points\"] = f\"About: {about} Key_points: {key_points}\"\n",
    "            company_data[\"company_links\"] = company_links\n",
    "        else:\n",
    "            company_data[\"about_and_key_points\"] = \"About not found. Key_points not found.\"\n",
    "            company_data[\"company_links\"] = []\n",
    "\n",
    "        # Return extracted data\n",
    "        return {\n",
    "            'company_name': company_name,\n",
    "            'stock_price': stock_price,\n",
    "            'percentage_change': percentage_change,\n",
    "            'ratios': ratios,\n",
    "            **company_data\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "# Extract company data\n",
    "company_data = extract_company_data(html)\n",
    "# Print extracted data\n",
    "if company_data is not None:\n",
    "    for key, value in company_data.items():\n",
    "        print(f\"{key.capitalize()}: {value}\")\n",
    "# Store extracted data in JSON format\n",
    "if company_data is not None:\n",
    "    with open('json/company_data.json', 'w') as f:\n",
    "        json.dump(company_data, f, indent=4)\n",
    "\n",
    "    print(\"Data stored in company_data.json\")\n",
    "else:\n",
    "    print(\"No data extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stored in quarters_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_quarters_data(html):\n",
    "    \"\"\"\n",
    "    Extract quarters data from the given HTML.\n",
    "\n",
    "    Args:\n",
    "        html (str): The HTML content to parse.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted quarters data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find the quarters section\n",
    "    quarters_section = soup.find('section', id='quarters')\n",
    "    if quarters_section is None:\n",
    "        print(\"Quarters section not found.\")\n",
    "        return None\n",
    "\n",
    "    # Find the table\n",
    "    table = quarters_section.find('table', class_='data-table')\n",
    "    if table is None:\n",
    "        print(\"Table not found.\")\n",
    "        return None\n",
    "\n",
    "    # Extract table headers\n",
    "    headers = [th.text.strip() for th in table.find_all('th')][1:]  # Skip the first column\n",
    "\n",
    "    # Extract table data\n",
    "    data = []\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [col.text.strip().replace(' ', '').replace('+', '').replace('<span class=\"blue-icon\">', '').replace('</span>', '').encode('ascii', 'ignore').decode('ascii') for col in cols][1:]  # Skip the first column\n",
    "        data.append([col for col in cols if col]) # Get rid of empty values\n",
    "\n",
    "    # Create a dictionary with the extracted data\n",
    "    categories = ['Sales', 'Expenses', 'Operating Profit', 'OPM %', 'Other Income', 'Interest', 'Depreciation', 'Profit before tax', 'Tax %', 'Net Profit', 'EPS in Rs']\n",
    "    quarters_data = {}\n",
    "    for i, header in enumerate(headers):\n",
    "        quarters_data[header] = {}\n",
    "        for j, category in enumerate(categories):\n",
    "            if j < len(data) and i < len(data[j]):\n",
    "                quarters_data[header][category] = data[j][i]\n",
    "            else:\n",
    "                quarters_data[header][category] = 'null'\n",
    "\n",
    "    return quarters_data\n",
    "\n",
    "\n",
    "# Extract quarters data\n",
    "quarters_data = extract_quarters_data(html)\n",
    "\n",
    "# Store extracted data in JSON format\n",
    "if quarters_data is not None:\n",
    "    with open('json/quarters_data.json', 'w') as f:\n",
    "        json.dump(quarters_data, f, indent=4)\n",
    "\n",
    "    print(\"Data stored in quarters_data.json\")\n",
    "else:\n",
    "    print(\"No data extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stored in profit_loss_data.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding projects\\Random_project\\dataExtraction\\datacrawler\\Lib\\site-packages\\bs4\\element.py:156: RuntimeWarning: coroutine 'simulate_button_clicks' was never awaited\n",
      "  def setup(self, parent=None, previous_element=None, next_element=None,\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data(html):\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Extract Profit & Loss data\n",
    "    profit_loss_section = soup.find('section', id='profit-loss')\n",
    "    if profit_loss_section is None:\n",
    "        print(\"Profit & Loss section not found.\")\n",
    "        return None\n",
    "\n",
    "    table = profit_loss_section.find('table', class_='data-table')\n",
    "    if table is None:\n",
    "        print(\"Table not found.\")\n",
    "        return None\n",
    "\n",
    "    headers = [th.text.strip() for th in table.find_all('th')][1:]  \n",
    "    data = []\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [col.text.strip().replace(' ', '').replace('+', '').replace('<span class=\"blue-icon\">', '').replace('</span>', '').encode('ascii', 'ignore').decode('ascii') for col in cols][1:]  \n",
    "        data.append([col for col in cols if col]) \n",
    "\n",
    "    categories = ['Sales', 'Expenses', 'Operating Profit', 'OPM %', 'Other Income', 'Interest', 'Depreciation', 'Profit before tax', 'Tax %', 'Net Profit', 'EPS in Rs']\n",
    "    profit_loss_data = {}\n",
    "    for i, header in enumerate(headers):\n",
    "        profit_loss_data[header] = {}\n",
    "        for j, category in enumerate(categories):\n",
    "            if j < len(data) and i < len(data[j]):\n",
    "                profit_loss_data[header][category] = data[j][i]\n",
    "            else:\n",
    "                profit_loss_data[header][category] = 'null'\n",
    "\n",
    "    # Extract Compounded Growth data\n",
    "    growth_tables = soup.find_all('table', class_='ranges-table')\n",
    "    if not growth_tables:\n",
    "        print(\"Growth tables not found.\")\n",
    "        return None\n",
    "\n",
    "    compounded_sales_growth = {}\n",
    "    compounded_profit_growth = {}\n",
    "    stock_price_cagr = {}\n",
    "    return_on_equity = {}\n",
    "\n",
    "    for i, table in enumerate(growth_tables):\n",
    "        rows = table.find_all('tr')[1:]  \n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            metric = cols[0].text.strip()\n",
    "            value = cols[1].text.strip()\n",
    "            if i == 0:\n",
    "                compounded_sales_growth[metric] = value\n",
    "            elif i == 1:\n",
    "                compounded_profit_growth[metric] = value\n",
    "            elif i == 2:\n",
    "                stock_price_cagr[metric] = value\n",
    "            elif i == 3:\n",
    "                return_on_equity[metric] = value\n",
    "\n",
    "    compounded_growth_data = {\n",
    "        'Compounded Sales Growth': compounded_sales_growth,\n",
    "        'Compounded Profit Growth': compounded_profit_growth,\n",
    "        'Stock Price CAGR': stock_price_cagr,\n",
    "        'Return on Equity': return_on_equity\n",
    "    }\n",
    "\n",
    "    # Combine data into a single dictionary\n",
    "    data = {\n",
    "        'Profit & Loss': profit_loss_data,\n",
    "        'Compounded Growth': compounded_growth_data\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Extract data\n",
    "data = extract_data(html)\n",
    "\n",
    "# Store extracted data in JSON format\n",
    "if data is not None:\n",
    "    with open('json/profit_loss_data.json', 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(\"Data stored in profit_loss_data.json\")\n",
    "else:\n",
    "    print(\"No data extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stored in balance_sheet_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_balance_sheet_data(html):\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find the balance sheet section\n",
    "    balance_sheet_section = soup.find('section', id='balance-sheet')\n",
    "    if balance_sheet_section is None:\n",
    "        print(\"Balance Sheet section not found.\")\n",
    "        return None\n",
    "\n",
    "    # Find the table\n",
    "    table = balance_sheet_section.find('table', class_='data-table')\n",
    "    if table is None:\n",
    "        print(\"Table not found.\")\n",
    "        return None\n",
    "\n",
    "    # Extract table headers\n",
    "    headers = [th.text.strip() for th in table.find_all('th')][1:]  \n",
    "\n",
    "    # Extract table data\n",
    "    data = []\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [col.text.strip().replace(' ', '').replace('+', '').replace('<span class=\"blue-icon\">', '').replace('</span>', '').replace('\\u00a0+', '').encode('ascii', 'ignore').decode('ascii') for col in cols][1:]  \n",
    "        data.append([col for col in cols if col]) \n",
    "\n",
    "    # Create a dictionary with the extracted data\n",
    "    balance_sheet_data = {}\n",
    "    categories = [row.find_all('td')[0].text.strip().replace('\\u00a0+', '') for row in table.find_all('tr')[1:]]\n",
    "    for i, header in enumerate(headers):\n",
    "        balance_sheet_data[header] = {}\n",
    "        for j, category in enumerate(categories):\n",
    "            if j < len(data) and i < len(data[j]):\n",
    "                balance_sheet_data[header][category] = data[j][i]\n",
    "            else:\n",
    "                balance_sheet_data[header][category] = 'null'\n",
    "\n",
    "    return balance_sheet_data\n",
    "\n",
    "\n",
    "# Extract balance sheet data\n",
    "balance_sheet_data = extract_balance_sheet_data(html)\n",
    "\n",
    "\n",
    "# Store extracted data in JSON format\n",
    "if balance_sheet_data is not None:\n",
    "    with open('json/balance_sheet_data.json', 'w') as f:\n",
    "        json.dump(balance_sheet_data, f, indent=4)\n",
    "\n",
    "    print(\"Data stored in balance_sheet_data.json\")\n",
    "else:\n",
    "    print(\"No data extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stored in cash_flows_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_cash_flows_data(html):\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find the cash flows section\n",
    "    cash_flows_section = soup.find('section', id='cash-flow')\n",
    "    if cash_flows_section is None:\n",
    "        print(\"Cash Flows section not found.\")\n",
    "        return None\n",
    "\n",
    "    # Find the table\n",
    "    table = cash_flows_section.find('table', class_='data-table')\n",
    "    if table is None:\n",
    "        print(\"Table not found.\")\n",
    "        return None\n",
    "\n",
    "    # Extract table headers\n",
    "    headers = [th.text.strip() for th in table.find_all('th')][1:]  \n",
    "\n",
    "    # Extract table data\n",
    "    data = []\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [col.text.strip().replace(' ', '').replace('+', '').replace('<span class=\"blue-icon\">', '').replace('</span>', '').replace('\\u00a0+', '').encode('ascii', 'ignore').decode('ascii') for col in cols][1:]  \n",
    "        data.append([col for col in cols if col]) \n",
    "\n",
    "    # Create a dictionary with the extracted data\n",
    "    cash_flows_data = {}\n",
    "    categories = [row.find_all('td')[0].text.strip().replace('\\u00a0+', '') for row in table.find_all('tr')[1:]]\n",
    "    for i, header in enumerate(headers):\n",
    "        cash_flows_data[header] = {}\n",
    "        for j, category in enumerate(categories):\n",
    "            if j < len(data) and i < len(data[j]):\n",
    "                cash_flows_data[header][category] = data[j][i]\n",
    "            else:\n",
    "                cash_flows_data[header][category] = 'null'\n",
    "\n",
    "    return cash_flows_data\n",
    "\n",
    "\n",
    "# Extract cash flows data\n",
    "cash_flows_data = extract_cash_flows_data(html)\n",
    "\n",
    "\n",
    "# Store extracted data in JSON format\n",
    "if cash_flows_data is not None:\n",
    "    with open('json/cash_flows_data.json', 'w') as f:\n",
    "        json.dump(cash_flows_data, f, indent=4)\n",
    "\n",
    "    print(\"Data stored in cash_flows_data.json\")\n",
    "else:\n",
    "    print(\"No data extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stored in ratios_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_ratios_data(html):\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find the ratios section\n",
    "    ratios_section = soup.find('section', id='ratios')\n",
    "    if ratios_section is None:\n",
    "        print(\"Ratios section not found.\")\n",
    "        return None\n",
    "\n",
    "    # Find the table\n",
    "    table = ratios_section.find('table', class_='data-table')\n",
    "    if table is None:\n",
    "        print(\"Table not found.\")\n",
    "        return None\n",
    "\n",
    "    # Extract table headers\n",
    "    headers = [th.text.strip() for th in table.find_all('th')][1:]  \n",
    "\n",
    "    # Extract table data\n",
    "    data = []\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [col.text.strip().replace(' ', '').replace('%', '').encode('ascii', 'ignore').decode('ascii') for col in cols][1:]  \n",
    "        data.append([col for col in cols if col]) \n",
    "\n",
    "    # Create a dictionary with the extracted data\n",
    "    ratios_data = {}\n",
    "    categories = [row.find_all('td')[0].text.strip() for row in table.find_all('tr')[1:]]\n",
    "    for i, header in enumerate(headers):\n",
    "        ratios_data[header] = {}\n",
    "        for j, category in enumerate(categories):\n",
    "            if j < len(data) and i < len(data[j]):\n",
    "                ratios_data[header][category] = data[j][i]\n",
    "            else:\n",
    "                ratios_data[header][category] = 'null'\n",
    "\n",
    "    return ratios_data\n",
    "\n",
    "\n",
    "# Extract ratios data\n",
    "ratios_data = extract_ratios_data(html)\n",
    "\n",
    "\n",
    "# Store extracted data in JSON format\n",
    "if ratios_data is not None:\n",
    "    with open('json/ratios_data.json', 'w') as f:\n",
    "        json.dump(ratios_data, f, indent=4)\n",
    "\n",
    "    print(\"Data stored in ratios_data.json\")\n",
    "else:\n",
    "    print(\"No data extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stored in shareholding_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_shareholding_data(html):\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find the shareholding section\n",
    "    shareholding_section = soup.find('section', id='shareholding')\n",
    "    if shareholding_section is None:\n",
    "        print(\"Shareholding section not found.\")\n",
    "        return None\n",
    "\n",
    "    # Find the quarterly and yearly tables\n",
    "    quarterly_table = shareholding_section.find('div', id='quarterly-shp').find('table')\n",
    "    yearly_table = shareholding_section.find('div', id='yearly-shp').find('table')\n",
    "\n",
    "    # Extract quarterly data\n",
    "    quarterly_data = {}\n",
    "    quarterly_headers = [th.text.strip() for th in quarterly_table.find_all('th')][1:]  \n",
    "    quarterly_rows = quarterly_table.find_all('tr')[1:]\n",
    "    for row in quarterly_rows:\n",
    "        cols = row.find_all('td')\n",
    "        category = cols[0].text.strip().replace('-', '').replace('+', '').replace('\\u00a0', '')\n",
    "        data = [col.text.strip().replace('-', '').replace('+', '').replace('\\u00a0', '') for col in cols[1:]]  \n",
    "        quarterly_data[category] = {}\n",
    "        for i, header in enumerate(quarterly_headers):\n",
    "            quarterly_data[category][header] = data[i]\n",
    "\n",
    "    # Extract yearly data\n",
    "    yearly_data = {}\n",
    "    yearly_headers = [th.text.strip() for th in yearly_table.find_all('th')][1:]  \n",
    "    yearly_rows = yearly_table.find_all('tr')[1:]\n",
    "    for row in yearly_rows:\n",
    "        cols = row.find_all('td')\n",
    "        category = cols[0].text.strip().replace('-', '').replace('+', '').replace('\\u00a0', '')\n",
    "        data = [col.text.strip().replace('-', '').replace('+', '').replace('\\u00a0', '') for col in cols[1:]]  \n",
    "        yearly_data[category] = {}\n",
    "        for i, header in enumerate(yearly_headers):\n",
    "            yearly_data[category][header] = data[i]\n",
    "\n",
    "    # Create a dictionary with the extracted data\n",
    "    shareholding_data = {\n",
    "        'Quarterly': quarterly_data,\n",
    "        'Yearly': yearly_data\n",
    "    }\n",
    "\n",
    "    return shareholding_data\n",
    "\n",
    "\n",
    "# Extract shareholding data\n",
    "shareholding_data = extract_shareholding_data(html)\n",
    "\n",
    "\n",
    "# Store extracted data in JSON format\n",
    "if shareholding_data is not None:\n",
    "    with open('json/shareholding_data.json', 'w') as f:\n",
    "        json.dump(shareholding_data, f, indent=4)\n",
    "\n",
    "    print(\"Data stored in shareholding_data.json\")\n",
    "else:\n",
    "    print(\"No data extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data stored in companies\\Sun Pharmaceuticals Industries Ltd.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define the path to the JSON files\n",
    "json_folder = 'json'\n",
    "companies_folder = 'companies'\n",
    "\n",
    "# Define the list of JSON files\n",
    "json_files = [\n",
    "    'company_data.json',\n",
    "    'quarters_data.json',\n",
    "    'balance_sheet_data.json',\n",
    "    'cash_flows_data.json',\n",
    "    'profit_loss_data.json',\n",
    "    'ratios_data.json',\n",
    "    'shareholding_data.json'\n",
    "]\n",
    "\n",
    "# Load company name from company_data.json\n",
    "\n",
    "# Load company data\n",
    "with open(os.path.join(json_folder, 'company_data.json'), 'r') as f:\n",
    "    company_data = json.load(f)\n",
    "    company_name = company_data['company_name']\n",
    "\n",
    "# Create a dictionary to store the combined data\n",
    "combined_data = {}\n",
    "\n",
    "# Iterate through the JSON files\n",
    "json_files = [f for f in os.listdir(json_folder) if f.endswith('.json') and f != 'company_data.json']\n",
    "\n",
    "for filename in json_files:\n",
    "    with open(os.path.join(json_folder, filename), 'r') as f:\n",
    "        data = json.load(f)\n",
    "        combined_data[filename.replace('.json', '')] = data\n",
    "\n",
    "# Add company name to combined data\n",
    "combined_data['company_name'] = company_name\n",
    "\n",
    "# Create a combined JSON file with company name\n",
    "company_file_path = os.path.join(companies_folder, f'{company_name}.json')\n",
    "\n",
    "with open(company_file_path, 'w') as f:\n",
    "    json.dump(combined_data, f, indent=4)\n",
    "\n",
    "print(f\"Combined data stored in {company_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import json\n",
    "# import os\n",
    "\n",
    "# # Define the path to the JSON folder\n",
    "# json_folder = 'json'\n",
    "\n",
    "# # Load company data from JSON file\n",
    "# with open(os.path.join(json_folder, 'company_data.json')) as f:\n",
    "#     company_data = json.load(f)\n",
    "\n",
    "# # Load quarters data from JSON file\n",
    "# with open(os.path.join(json_folder, 'quarters_data.json')) as f:\n",
    "#     quarters_data = json.load(f)\n",
    "    \n",
    "# # Extract company data\n",
    "\n",
    "# def print_company_data(company_data):\n",
    "#     for key, value in company_data.items():\n",
    "#         print(f\"{key.capitalize()}: {value}\")\n",
    "\n",
    "# def print_quarters_data(quarters_data):\n",
    "#     for key, value in quarters_data.items():\n",
    "#         print(f\"{key}:\")\n",
    "#         for k, v in value.items():\n",
    "#             print(f\"  {k}: {v}\")\n",
    "            \n",
    "# def store_company_data_in_csv(company_data, filename):\n",
    "#     with open(filename, 'w', newline='') as csvfile:\n",
    "#         writer = csv.writer(csvfile)\n",
    "        \n",
    "#         # Store company data\n",
    "#         writer.writerow([\"Category\", \"Key\", \"Value\"])  # header\n",
    "#         for key, value in company_data.items():\n",
    "#             if key not in [\"ratios\", \"company_links\", \"about_and_key_points\"]:\n",
    "#                 writer.writerow([\"Company\", key, value])\n",
    "        \n",
    "#         # Store ratios\n",
    "#         writer.writerow([\"Category\", \"Key\", \"Value\"])  # header\n",
    "#         for ratio_key, ratio_value in company_data[\"ratios\"].items():\n",
    "#             writer.writerow([\"Ratios\", ratio_key, ratio_value])\n",
    "        \n",
    "#         # Store company links\n",
    "#         writer.writerow([\"Category\", \"Key\", \"Value\"])  # header\n",
    "#         for link_key, link_value in enumerate(company_data[\"company_links\"]):\n",
    "#             writer.writerow([\"Company Links\", f\"Link {link_key+1}\", link_value])\n",
    "        \n",
    "#         # Store about and key points\n",
    "#         writer.writerow([\"Category\", \"Key\", \"Value\"])  # header\n",
    "#         about_and_key_points = company_data[\"about_and_key_points\"].split(\"Key_points: \")\n",
    "#         about = about_and_key_points[0]\n",
    "#         key_points = about_and_key_points[1]\n",
    "#         writer.writerow([\"About\", \"\", about])\n",
    "#         writer.writerow([\"Key Points\", \"\", key_points])\n",
    "\n",
    "\n",
    "\n",
    "# def store_quarters_data_in_csv(quarters_data, filename):\n",
    "#     with open(filename, 'w', newline='') as csvfile:\n",
    "#         writer = csv.writer(csvfile)\n",
    "        \n",
    "#         # Write header\n",
    "#         categories = list(quarters_data.keys())\n",
    "#         quarters = list(quarters_data[categories[0]].keys())\n",
    "#         writer.writerow([\"Category\"] + quarters)  # header\n",
    "        \n",
    "#         # Write data\n",
    "#         for category in categories:\n",
    "#             row = [category]\n",
    "#             for quarter in quarters:\n",
    "#                 if quarter in quarters_data[category]:\n",
    "#                     row.append(quarters_data[category][quarter])\n",
    "#                 else:\n",
    "#                     row.append(\"null\")  # Use \"null\" as a placeholder\n",
    "#             writer.writerow(row)\n",
    "            \n",
    "\n",
    "# def store_data_in_csv(company_data, quarters_data, filename):\n",
    "#     with open(filename, 'w', newline='') as csvfile:\n",
    "#         writer = csv.writer(csvfile)\n",
    "        \n",
    "#         # Store company data\n",
    "#         writer.writerow([\"Category\", \"Key\", \"Value\"])  # header\n",
    "#         for key, value in company_data.items():\n",
    "#             if key not in [\"ratios\", \"company_links\", \"about_and_key_points\"]:\n",
    "#                 writer.writerow([\"Company\", key, value])\n",
    "        \n",
    "#         # Store ratios\n",
    "#         for ratio_key, ratio_value in company_data[\"ratios\"].items():\n",
    "#             writer.writerow([\"Ratios\", ratio_key, ratio_value])\n",
    "        \n",
    "#         # Store company links\n",
    "#         for link_key, link_value in enumerate(company_data[\"company_links\"]):\n",
    "#             writer.writerow([\"Company Links\", f\"Link {link_key+1}\", link_value])\n",
    "        \n",
    "#         # Store about and key points\n",
    "#         about_and_key_points = company_data[\"about_and_key_points\"].split(\"Key_points: \")\n",
    "#         about = about_and_key_points[0]\n",
    "#         key_points = about_and_key_points[1]\n",
    "#         writer.writerow([\"About\", \"\", about])\n",
    "#         writer.writerow([\"Key Points\", \"\", key_points])\n",
    "        \n",
    "#         # Store quarters data\n",
    "#         writer.writerow([\"Quarters Data\"])  # header\n",
    "#         categories = list(quarters_data.keys())\n",
    "#         quarters = list(quarters_data[categories[0]].keys())\n",
    "#         writer.writerow([\"Category\"] + quarters)  # header\n",
    "#         for category in categories:\n",
    "#             row = [category]\n",
    "#             for quarter in quarters:\n",
    "#                 if quarter in quarters_data[category]:\n",
    "#                     row.append(quarters_data[category][quarter])\n",
    "#                 else:\n",
    "#                     row.append(\"null\")  # Use \"null\" as a placeholder\n",
    "#             writer.writerow(row)\n",
    "\n",
    "# def load_json_data(file_name):\n",
    "#     with open(os.path.join(json_folder, file_name)) as f:\n",
    "#         return json.load(f)\n",
    "\n",
    "# # Load company data\n",
    "# company_data = load_json_data('company_data.json')\n",
    "\n",
    "# # Load quarters data\n",
    "# quarters_data = load_json_data('quarters_data.json')\n",
    "# # Print the data\n",
    "# if company_data is not None:\n",
    "#     print_company_data(company_data)\n",
    "# if quarters_data is not None:\n",
    "#     print_quarters_data(quarters_data)\n",
    "\n",
    "# # Store the data in a CSV file\n",
    "# if company_data is not None or quarters_data is not None:\n",
    "#     store_data_in_csv(company_data, quarters_data, 'company_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from bs4 import BeautifulSoup\n",
    "# from requests_html import HTMLSession\n",
    "# import asyncio\n",
    "# from requests_html import AsyncHTMLSession\n",
    "\n",
    "\n",
    "# async def simulate_button_clicks(url):\n",
    "#     session = AsyncHTMLSession()\n",
    "#     r = await session.get(url)\n",
    "#     await r.html.arender()\n",
    "\n",
    "#     # Find buttons and simulate clicks\n",
    "#     buttons = r.html.find('.button-plain')\n",
    "#     for button in buttons:\n",
    "#         # Check if button has onclick attribute\n",
    "#         if button.attrs.get('onclick'):\n",
    "#             # Execute JavaScript onclick code\n",
    "#             await r.html.arender(script=button.attrs.get('onclick'))\n",
    "#         # Check if button has href attribute\n",
    "#         elif button.attrs.get('href'):\n",
    "#             # Send GET request to href URL\n",
    "#             r = await session.get(button.attrs.get('href'))\n",
    "#             await r.html.arender()\n",
    "\n",
    "#     return r.html.html\n",
    "\n",
    "\n",
    "# def extract_shareholding_data(html_shareholder):\n",
    "#     # Parse HTML content\n",
    "#     soup = BeautifulSoup(html_shareholder, 'html.parser')\n",
    "\n",
    "#     # Find the shareholding section\n",
    "#     shareholding_section = soup.find('section', id='shareholding')\n",
    "#     if shareholding_section is None:\n",
    "#         print(\"Shareholding section not found.\")\n",
    "#         return None\n",
    "\n",
    "#     # Find the quarterly and yearly tables\n",
    "#     quarterly_table = shareholding_section.find('div', id='quarterly-shp').find('table')\n",
    "#     yearly_table = shareholding_section.find('div', id='yearly-shp').find('table')\n",
    "\n",
    "#     # Extract quarterly data\n",
    "#     quarterly_data = {}\n",
    "#     quarterly_headers = [th.text.strip() for th in quarterly_table.find_all('th')][1:]  \n",
    "#     quarterly_rows = quarterly_table.find_all('tr')\n",
    "\n",
    "#     for row in quarterly_rows:\n",
    "#         cols = row.find_all('td')\n",
    "#         if len(cols) < 2:\n",
    "#             continue\n",
    "\n",
    "#         category = cols[0].text.strip().replace('-', '').replace('+', '').replace('\\u00a0', '')\n",
    "#         if category == '':\n",
    "#             continue\n",
    "\n",
    "#         data = [col.text.strip().replace('-', '').replace('+', '').replace('\\u00a0', '') for col in cols[1:]]  \n",
    "#         quarterly_data[category] = {}\n",
    "#         for i, header in enumerate(quarterly_headers):\n",
    "#             if i < len(data):\n",
    "#                 quarterly_data[category][header] = data[i]\n",
    "\n",
    "#     # Extract yearly data\n",
    "#     yearly_data = {}\n",
    "#     yearly_headers = [th.text.strip() for th in yearly_table.find_all('th')][1:]  \n",
    "#     yearly_rows = yearly_table.find_all('tr')\n",
    "\n",
    "#     for row in yearly_rows:\n",
    "#         cols = row.find_all('td')\n",
    "#         if len(cols) < 2:\n",
    "#             continue\n",
    "\n",
    "#         category = cols[0].text.strip().replace('-', '').replace('+', '').replace('\\u00a0', '')\n",
    "#         if category == '':\n",
    "#             continue\n",
    "\n",
    "#         data = [col.text.strip().replace('-', '').replace('+', '').replace('\\u00a0', '') for col in cols[1:]]  \n",
    "#         yearly_data[category] = {}\n",
    "#         for i, header in enumerate(yearly_headers):\n",
    "#             if i < len(data):\n",
    "#                 yearly_data[category][header] = data[i]\n",
    "\n",
    "#     # Create a dictionary with the extracted data\n",
    "#     shareholding_data = {\n",
    "#         'Quarterly': quarterly_data,\n",
    "#         'Yearly': yearly_data\n",
    "#     }\n",
    "\n",
    "#     return shareholding_data\n",
    "\n",
    "\n",
    "# # Simulate button clicks and extract shareholding data\n",
    "# url = 'https://www.screener.in/company/SUNPHARMA/consolidated/'\n",
    "# html_shareholder = asyncio.run(simulate_button_clicks(url))\n",
    "\n",
    "# if html_shareholder:\n",
    "#     shareholding_data = extract_shareholding_data(html_shareholder)\n",
    "    \n",
    "#     # Store extracted data in JSON format\n",
    "#     if shareholding_data is not None:\n",
    "#         with open('json/shareholding_data.json', 'w') as f:\n",
    "#             json.dump(shareholding_data, f, indent=4)\n",
    "\n",
    "#         print(\"Data stored in shareholding_data.json\")\n",
    "#     else:\n",
    "#         print(\"No data extracted.\")\n",
    "# else:\n",
    "#     print(\"Failed to retrieve HTML content.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacrawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
